{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Custom Conversational Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I implement a basic seq2seq conversational chatbot. My work here is somewhat like [the Deep Q&A project](https://github.com/Conchylicultor/DeepQA), but I use [Keras](https://keras.io/) as a neural network framework because it is so much cleaner and easier than the underlying TensorFlow. For the dataset, I choose [Cornell Movie-Dialogs Corpus](https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html) which provides adequate dialogues for building a simple chit-chat system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/rawpixel-633846-unsplash.jpg\" alt=\"Buiding a Bot\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chatbot implementation can be breaked down into 5 steps as follows.\n",
    "<ol>\n",
    "      <li>Read text data</li>\n",
    "      <li>Transform the texts into a format that is proper to be used by the training module</li>\n",
    "      <li>Build a basic seq2seq model</li>\n",
    "      <li>Train the model by using prepared dialogue data</li>\n",
    "      <li>Evaluate the trained model</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '2'\n",
    "\n",
    "import numpy as np\n",
    "import yaml, pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the section that sets values of important parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config_name = 'config02'\n",
    "\n",
    "with open('{}.yml'.format(config_name)) as config_file:\n",
    "    configs = yaml.load(config_file)\n",
    "\n",
    "MAX_LEN = configs['params']['sequence']['max_len']\n",
    "START_TOKEN = configs['params']['sequence']['start_token']\n",
    "KEPT_SYMBOLS = configs['params']['sequence']['kept_symbols']\n",
    "\n",
    "NUM_WORDS = configs['params']['tokenizer']['num_words']    \n",
    "OOV_TOKEN = configs['params']['tokenizer']['oov_token']\n",
    "LOWER = configs['params']['tokenizer']['lower']\n",
    "FILTERS = configs['params']['tokenizer']['filters']\n",
    "\n",
    "EMBEDDING_DIM = configs['params']['model']['embedding_dim']\n",
    "STATE_DIM = configs['params']['model']['state_dim']\n",
    "\n",
    "BATCH_SIZE = configs['params']['training']['batch_size']\n",
    "EPOCHS = configs['params']['training']['epochs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original dataset has been processed by [ParlAI](http://www.parl.ai/), resulted in ready-to-use data files which are already in the tab-separated format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# paths to the data txt files on disk\n",
    "train_path = '../data/CornellMovie/train.txt'\n",
    "valid_path = '../data/CornellMovie/valid.txt'\n",
    "test_path = '../data/CornellMovie/test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def process_symbol(texts):\n",
    "    if not KEPT_SYMBOLS:\n",
    "        return texts\n",
    "    for s in KEPT_SYMBOLS:\n",
    "        texts = [text.replace(s, ' {} '.format(s)) for text in texts]        \n",
    "    return texts\n",
    "\n",
    "def read_text(text_path):\n",
    "    input_texts = []\n",
    "    target_texts = []\n",
    "\n",
    "    with open(text_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.read().split('\\n')\n",
    "\n",
    "    for line in lines:\n",
    "        if not line:\n",
    "            continue\n",
    "        \n",
    "        line = line[line.find(' ')+1:]\n",
    "        \n",
    "        if len(line.split('\\t')) != 2:\n",
    "            continue\n",
    "            \n",
    "        input_text, target_text = line.split('\\t')\n",
    "        \n",
    "        input_texts.append(input_text)\n",
    "        target_texts.append(target_text)\n",
    "        \n",
    "    with open(text_path.replace('.txt', '_inputs.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(input_texts))\n",
    "              \n",
    "    with open(text_path.replace('.txt', '_targets.txt'), 'w') as f:\n",
    "        f.write('\\n'.join(target_texts))\n",
    "        \n",
    "    input_texts = process_symbol(input_texts)\n",
    "    target_texts = process_symbol(target_texts)\n",
    "\n",
    "    print(\"{}\".format(text_path))\n",
    "    for i in range(3):\n",
    "        print(\"Input_{}: {}\".format(i, input_texts[i]))\n",
    "        print(\"Target_{}: {}\".format(i, target_texts[i]))\n",
    "    print()            \n",
    "    \n",
    "    return input_texts, target_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read and take a glimpse at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/CornellMovie/train.txt\n",
      "Input_0: You're asking me out .   That's so cute .  What's your name again ? \n",
      "Target_0: Forget it . \n",
      "Input_1: No ,  no ,  it's my fault -- we didn't have a proper introduction ---\n",
      "Target_1: Cameron . \n",
      "Input_2: The thing is ,  Cameron -- I'm at the mercy of a particularly hideous breed of loser .   My sister .   I can't date until she does . \n",
      "Target_2: Seems like she could get a date easy enough .  .  . \n",
      "\n",
      "../data/CornellMovie/valid.txt\n",
      "Input_0: Can we make this quick ?   Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad .   Again . \n",
      "Target_0: Well ,  I thought we'd start with pronunciation ,  if that's okay with you . \n",
      "Input_1: Not the hacking and gagging and spitting part .   Please . \n",
      "Target_1: Okay .  .  .  then how 'bout we try out some French cuisine .   Saturday ?   Night ? \n",
      "Input_2: How do you get your hair to look like that ? \n",
      "Target_2: Eber's Deep Conditioner every two days .  And I never ,  ever use a blowdryer without the diffuser attachment . \n",
      "\n",
      "../data/CornellMovie/test.txt\n",
      "Input_0: You have my word .   As a gentleman\n",
      "Target_0: You're sweet . \n",
      "Input_1: do you listen to this crap ? \n",
      "Target_1: What crap ? \n",
      "Input_2: Me .   This endless  .  .  . blonde babble .  I'm like ,  boring myself . \n",
      "Target_2: Thank God !   If I had to hear one more story about your coiffure .  .  . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_text_inputs, train_text_targets = read_text(train_path)\n",
    "valid_text_inputs, valid_text_targets = read_text(valid_path)\n",
    "test_text_inputs, test_text_targets = read_text(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the teacher forcing scheme, inputs to a decoder begin with the start symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> Forget it . \n",
      "<start> Well ,  I thought we'd start with pronunciation ,  if that's okay with you . \n",
      "<start> You're sweet . \n"
     ]
    }
   ],
   "source": [
    "def insert_start(texts):\n",
    "    return [START_TOKEN + ' ' + text for text in texts]\n",
    "\n",
    "train_text_targets_with_start = insert_start(train_text_targets)\n",
    "valid_text_targets_with_start = insert_start(valid_text_targets)\n",
    "test_text_targets_with_start = insert_start(test_text_targets)\n",
    "\n",
    "print(train_text_targets_with_start[0])\n",
    "print(valid_text_targets_with_start[0])\n",
    "print(test_text_targets_with_start[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The raw texts must be tokenized and vectorized before entering the seq2seq networks. Keras also equips with some text processing modules which facilitate this kind of tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "tokenizer = Tokenizer(num_words=NUM_WORDS, oov_token=OOV_TOKEN, lower=LOWER, filters=FILTERS)\n",
    "tokenizer.fit_on_texts(train_text_inputs + train_text_targets_with_start)\n",
    "\n",
    "with open('../models/tokenizer_{}.pkl'.format(config_name), 'wb') as handle:\n",
    "    pickle.dump(tokenizer, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer is fitted with training texts and now can be used to transform the texts into sequences that will enter a seq2seq model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word2index = tokenizer.word_index\n",
    "index2word = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "def transform_text(texts):\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    data = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_inputs = transform_text(train_text_inputs)\n",
    "valid_data_inputs = transform_text(valid_text_inputs)\n",
    "test_data_inputs = transform_text(test_text_inputs)\n",
    "\n",
    "train_data_targets_with_start = transform_text(train_text_targets_with_start)\n",
    "valid_data_targets_with_start = transform_text(valid_text_targets_with_start)\n",
    "test_data_targets_with_start = transform_text(test_text_targets_with_start)\n",
    "\n",
    "train_data_targets = transform_text(train_text_targets)[:, :, np.newaxis]\n",
    "valid_data_targets = transform_text(valid_text_targets)[:, :, np.newaxis]\n",
    "test_data_targets = transform_text(test_text_targets)[:, :, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How is our little Find the Wench A Date plan progressing ? \n",
      "[   93    18   143   127  1991     7 33323   113 10791   737 19670     4\n",
      "     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "print(train_text_inputs[7])\n",
    "print(train_data_inputs[7])\n",
    "\n",
    "#print(train_text_targets[7])\n",
    "#print(train_data_targets[7])\n",
    "\n",
    "#print(train_text_targets_with_start[7])\n",
    "#print(train_data_targets_with_start[7])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Basic Seq2Seq Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A basic seq2seq model comprises of two RNN streams, an encoder and a decoder. The encoder receives chat texts and gathers information to the decoder. The decoder processes this information and generate replies. The model can be illustrated as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/seq2seq.png\" alt=\"A Basic Seq2Seq Model\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concept can be translated to the programming code by using Keras functional APIs. The implementation here is essentially a modification of the code from [this Keras official blog](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_inputs (InputLayer)     (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     (None, None, 300)    18329700    encoder_inputs[0][0]             \n",
      "                                                                 decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "encoder_rnn (CuDNNLSTM)         [(None, 512), (None, 1667072     embedding_layer[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_rnn (CuDNNLSTM)         [(None, None, 512),  1667072     embedding_layer[1][0]            \n",
      "                                                                 encoder_rnn[0][1]                \n",
      "                                                                 encoder_rnn[0][2]                \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 61099)  31343787    decoder_rnn[0][0]                \n",
      "==================================================================================================\n",
      "Total params: 53,007,631\n",
      "Trainable params: 53,007,631\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Embedding, CuDNNLSTM, Dense\n",
    "\n",
    "vocab_size = NUM_WORDS if NUM_WORDS else len(word2index)    \n",
    "\n",
    "# shared embedder\n",
    "embedding_layer = Embedding(vocab_size, EMBEDDING_DIM, name='embedding_layer')\n",
    "\n",
    "# encoder\n",
    "encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
    "encoder_rnn = CuDNNLSTM(STATE_DIM, return_state=True, name='encoder_rnn')\n",
    "\n",
    "x = embedding_layer(encoder_inputs)\n",
    "encoder_outputs, state_h, state_c = encoder_rnn(x)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "#decoder\n",
    "decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
    "decoder_rnn = CuDNNLSTM(STATE_DIM, return_sequences=True, return_state=True, name='decoder_rnn')\n",
    "decoder_dense = Dense(vocab_size, activation='softmax', name='decoder_dense')\n",
    "\n",
    "y = embedding_layer(decoder_inputs)\n",
    "y, _, _ = decoder_rnn(y, initial_state=encoder_states)\n",
    "decoder_outputs = decoder_dense(y)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the model is able to be trained with prepared data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "model_file = '../models/seq2seq-cornell_{}.hdf5'.format(config_name)\n",
    "log_file = '../models/seq2seq-cornell_{}.txt'.format(config_name)\n",
    "\n",
    "checkpoint = ModelCheckpoint(model_file)\n",
    "csv_logger = CSVLogger(log_file, append=True, separator=',')\n",
    "\n",
    "callbacks_list = [checkpoint, csv_logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(), loss='sparse_categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training process can be time-consuming, up to more than 10 hours on Tesla K80 GPU, for running 100 epochs of this dataset with about 50M model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit([train_data_inputs, train_data_targets_with_start], train_data_targets,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS, \n",
    "          callbacks=callbacks_list,\n",
    "          validation_data=([valid_data_inputs, valid_data_targets_with_start], valid_data_targets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the training has finished, the trained model is ready to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.evaluate([test_data_inputs, test_data_targets_with_start], test_data_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict([test_data_inputs, test_data_targets_with_start])\n",
    "for p in predictions[:10]:\n",
    "    words = []\n",
    "    for index in np.argmax(p, axis=-1):\n",
    "        if index == 0:\n",
    "            break\n",
    "        words.append(index2word[index])\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use in real-world situations, we don't know the complete reply sentence in advance. So, the decoder must receive the previous predicted word as an input, together with the previous state. The trained model needs to be modified a little bit and the decoder must operate step by step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_inputs (InputLayer)  (None, None)              0         \n",
      "_________________________________________________________________\n",
      "embedding_layer (Embedding)  (None, None, 300)         18329700  \n",
      "_________________________________________________________________\n",
      "encoder_rnn (CuDNNLSTM)      [(None, 512), (None, 512) 1667072   \n",
      "=================================================================\n",
      "Total params: 19,996,772\n",
      "Trainable params: 19,996,772\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_inputs (InputLayer)     (None, None)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_layer (Embedding)     (None, None, 300)    18329700    decoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_rnn (CuDNNLSTM)         [(None, None, 512),  1667072     embedding_layer[2][0]            \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 61099)  31343787    decoder_rnn[1][0]                \n",
      "==================================================================================================\n",
      "Total params: 51,340,559\n",
      "Trainable params: 51,340,559\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "encoder_model.summary()\n",
    "\n",
    "decoder_state_input_h = Input(shape=(STATE_DIM,))\n",
    "decoder_state_input_c = Input(shape=(STATE_DIM,))\n",
    "decoder_state_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "y = embedding_layer(decoder_inputs)\n",
    "y, state_h, state_c = decoder_rnn(y, initial_state=decoder_state_inputs)\n",
    "decoder_outputs = decoder_dense(y)\n",
    "decoder_state_outputs = [state_h, state_c]\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_state_inputs,\n",
    "    [decoder_outputs] + decoder_state_outputs)\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    target_seq = np.array([[tokenizer.word_index[START_TOKEN]]])\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = []   \n",
    "    \n",
    "    word_count = 0\n",
    "    \n",
    "    while True:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        \n",
    "        if (sampled_token_index == 0 or word_count >= MAX_LEN):\n",
    "            break\n",
    "            \n",
    "        decoded_sentence.append(index2word[sampled_token_index])\n",
    "        word_count += 1\n",
    "        \n",
    "        states_value = [h, c]\n",
    "        target_seq = np.array([[sampled_token_index]])  \n",
    "\n",
    "    return ' '.join(decoded_sentence)\n",
    "\n",
    "def format_text(text):\n",
    "    if not KEPT_SYMBOLS:\n",
    "        return text    \n",
    "    for s in KEPT_SYMBOLS:\n",
    "        text = text.replace(' {}'.format(s), s)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jobs Done! We can chat to the bot with any questions. Let's test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Hello!\n",
      "A: Well? Tell us! How'd it go?\n",
      "Q: How are you?\n",
      "A: Not bad.\n",
      "Q: Who are you?\n",
      "A: That would be difficult to explain.\n",
      "Q: What is your name?\n",
      "A: Bedevere, my Liege.\n",
      "Q: How old are you?\n",
      "A: Older.\n",
      "Q: Tell me a joke.\n",
      "A: Tell him there was Miss Rossi in the other name of the corner. Why\n",
      "Q: What time is it?\n",
      "A: Eleven o'clock... I'll be back later.\n",
      "Q: Who is Skywalker?\n",
      "A: Yes, sir.\n",
      "Q: What is immoral?\n",
      "A: You're not going to let it!\n",
      "Q: What is morality?\n",
      "A: One of the plagues on members of Congress office going out front of the Creator\n",
      "Q: What is the purpose of existence?\n",
      "A: The numbers stop in the country. I never noticed it, but it is\n",
      "Q: What is the purpose of being intelligent?\n",
      "A: Everything. Why do you think I thought it was a really great thing about\n",
      "Q: What happens if machines can think?\n",
      "A: If we go for fun, are you?\n",
      "Q: Do you prefer cats or dogs?\n",
      "A: I'll be fine.\n",
      "Q: I play tennis. What do you play?\n",
      "A: Nothing.\n"
     ]
    }
   ],
   "source": [
    "texts = [\"Hello!\",\n",
    "         \"How are you?\",\n",
    "         \"Who are you?\",\n",
    "         \"What is your name?\",\n",
    "         \"How old are you?\",\n",
    "         \"Tell me a joke.\",\n",
    "         \"What time is it?\",\n",
    "         \"Who is Skywalker?\",\n",
    "         \"What is immoral?\",\n",
    "         \"What is morality?\",\n",
    "         \"What is the purpose of existence?\",         \n",
    "         \"What is the purpose of being intelligent?\",         \n",
    "         \"What happens if machines can think?\",\n",
    "         \"Do you prefer cats or dogs?\",\n",
    "         \"I play tennis. What do you play?\"]\n",
    "\n",
    "input_data = transform_text(process_symbol(texts))\n",
    "\n",
    "for text, data in zip(texts, input_data):\n",
    "    decoded_sentence = format_text(decode_sequence(data[np.newaxis, :]))\n",
    "    print(\"Q: {}\".format(text))\n",
    "    print(\"A: {}\".format(decoded_sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Remarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<ul>\n",
    "    <li> I've found that in this project, using validation data to evaluate a model results in underfitting. When choosing the model at the epoch which evaluation score is optimal, the bot often replies with \"i don't know\" answer. However, if the model is continued training until the training loss is low enough, the quality of replies is better. </li>\n",
    "    <li> It's also interesting that the undertraining model can output \"i don't know\" answer as if it really doesn't understand a question. </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<ul>\n",
    "    <li> With large vocabulary size and long sequence length, for example 60000 vocabularies at the lenght of 20, training loss  turns to be nan at some point and the model cannot further be trained. Some regularization techniques may solve this problem. </li>\n",
    "    <li> At some settings, validation loss becomes nan but training loss does not.\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li> There is still a lot of room for model improvement, for example\n",
    "        <ul>\n",
    "            <li>reversing an input sequence and padding at the front instead</li>\n",
    "            <li>improving the tokenizer to not just use only a space as a separator</li>\n",
    "            <li>adding the attention mechanism</li>\n",
    "            <li>adding more layers to the RNN</li>\n",
    "            <li>using pretrained embeddings, e.g. GloVe</li>\n",
    "            <li>maybe sharing an embedding layer for the output layer too</li>\n",
    "            <li>adding regularization methods to the model, such as dropout and batch normalization</li>\n",
    "        </ul>\n",
    "    </li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<ul>\n",
    "    <li> Actually, this is still far from a rational intelligent conversational agent. It lacks consistency, persona, world knowledge and many aspects which any practical chatbots should have. Nonetheless, this project demonstrates that a seq2seq model, even in the most basic form, with proper data and parameters could produce rather sensible interactions with human users. </li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
